{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cbf51a2a14c586d5291331e7296f10f4c57288ef364409bd6d5208dc30c7c035"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteker\n",
    "# kjør \"generate_vocab.py\" før du kjører denne filen\n",
    "\n",
    "\"\"\"\n",
    "bruker tensorflow nightly:\n",
    "\n",
    "pip3 install -q tensorflow_text_nightly --user\n",
    "pip3 install -q tf-nightly --user\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 7641 files belonging to 7 classes.\n",
      "Using 6113 files for training.\n",
      "Found 7641 files belonging to 7 classes.\n",
      "Using 1528 files for validation.\n",
      "Found 1000 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# importerer og blander dataen\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# splitter treningsdataen i to, en til trening og en til validering av treningen\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"./data/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/test',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skaffer vokabularet vi har laget\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "bert_vocab_args = dict(\n",
    "    # maksimum størrelse for vokabularet\n",
    "    vocab_size = 8000 * 7,\n",
    "    # Reserverte orddeler som må være med\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # flere argumenter\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "# lager en \"tokenizer\", som deler tekst opp i orddeler\n",
    "tokenizer = text.BertTokenizer('vocab.txt', **bert_tokenizer_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjoner som blir brukt til å voktorisere teksten\n",
    "# altså gjøre dem om til tall i stedet for bokstaver\n",
    "# hvert tall er IDen til en orddel fra vokabularet vårt\n",
    "\n",
    "# alle vektorene må være like lange, så vi legger til\n",
    "# \"Padding\" på slutten av vektoren hvis den ikke er lang nok\n",
    "\n",
    "# IDen til padding\n",
    "PAD_ID = 0\n",
    "# Maksimum lengde for vektoren\n",
    "# hvis vektoren er mindre, blir det lagt til padding\n",
    "max_seq_len = 20\n",
    "\n",
    "# denne funskjonen er her for å passe formatet vi skal bruke den i senere\n",
    "def vectorize(text, label):\n",
    "  # text er tekst inputtet, og label er hvilket språk det er på\n",
    "  ids, mask, type_ids = preprocess_bert_input(text)\n",
    "  return (ids, mask, type_ids), label\n",
    "\n",
    "def preprocess_bert_input(text):\n",
    "  # finner IDene til alle orddelene i inputtet\n",
    "  ids = tokenize_text(text, max_seq_len)\n",
    "  # lager en mask, som i dette tilfettet representerer lengden på vektoren vår\n",
    "  mask = tf.cast(ids > 0, tf.int64)\n",
    "  mask = tf.reshape(mask, [-1, max_seq_len])\n",
    "  # lager den ferdige vektoren\n",
    "  # først fyller lager vi en vektor med\n",
    "  # den riktige lengden (shape) fyllt med nuller\n",
    "  zeros_dims = tf.stack(tf.shape(mask))\n",
    "  type_ids = tf.fill(zeros_dims, PAD_ID)\n",
    "  # så setter vi inn de faktiske orddelenes IDer\n",
    "  type_ids = tf.cast(type_ids, tf.int64)\n",
    "\n",
    "  return (ids, mask, type_ids)\n",
    "\n",
    "def tokenize_text(text, seq_len):\n",
    "  # bruker \"tokenizeren\" vi lagde tidligere til å generere tokens som passer teksten\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  # tilpasser outputtet\n",
    "  tokens = tokens.merge_dims(1, 2)[:, :seq_len]\n",
    "\n",
    "  # klipper vekk slutten hvis den er lenger enn maksimum lengde\n",
    "  tokens = tokens[:, :seq_len]\n",
    "  # legger til padding hvis den er kortere enn maksimum lengde\n",
    "  tokens = tokens.to_tensor(default_value=PAD_ID)\n",
    "  pad = seq_len - tf.shape(tokens)[1]\n",
    "  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n",
    "  return tf.reshape(tokens, [-1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vektoriserer hvert datasett, med funksjonen vi lagde med det spesielle formatet\n",
    "train_ds = raw_train_ds.map(vectorize)\n",
    "val_ds = raw_val_ds.map(vectorize)\n",
    "test_ds = raw_test_ds.map(vectorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tekst:  tf.Tensor(b'den koloniale bandet blei i r\\xc3\\xb8ynda brote i 1808 d\\xc3\\xa5 hovudstaden i det portugisiske koloniriket blei overf\\xc3\\xb8rt fr\\xc3\\xa5 lisboa til rio de janeiro etter napoleon sin invasjon av portugal', shape=(), dtype=string)\nSpråk:  nn\nVektorisert:  tf.Tensor(\n[[ 248   24 1805 3454 2312 3539  284 1719   22   31 3519  684  245   15\n   464  277   22 1712  383  471]], shape=(1, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# printer en bit av dataen for å se at det fungerer\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Tekst: \", first_review)\n",
    "print(\"Språk: \", raw_train_ds.class_names[first_label])\n",
    "print(\"Vektorisert: \", vectorize(first_review, first_label)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimaliserer dataen (er ærlig talt ikke sikker på hva dette gjør men alle andre prosjekter gjør det)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 16)            160000    \n_________________________________________________________________\ndropout (Dropout)            (None, 20, 16)            0         \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 16)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 16)                0         \n_________________________________________________________________\ndense (Dense)                (None, 7)                 119       \n=================================================================\nTotal params: 160,119\nTrainable params: 160,119\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# designer modellen som skal bli lært\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  # omformer inputtet til tall som er \"bedre å lære med\"\n",
    "  # i stedet for et heltall som ID for hver orddel, blir det til en liste\n",
    "  # med nuller for hvert språk det ikke er og en 1 for det språket som er riktig\n",
    "  # dette gjør at modellen ikke tror at orddeler som har tall nærme hverandre likner hverandre\n",
    "  layers.Embedding(10000, 16, input_length=max_seq_len),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # passer på at dataen forstsatt har lik lengde\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # output (en for hvert språk, som det er 7 av)\n",
    "  layers.Dense(7)])\n",
    "\n",
    "# printer ut modellen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompilerer modellen\n",
    "# optimisereren er en funksjon som proøver å forbedre modellen\n",
    "# i hver iterasjon av læringen\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(\n",
    "    # loss funksjonen beregner hvor langt unna det riktige svaret modellen er\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=optimizer, metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "192/192 [==============================] - 15s 76ms/step - loss: 1.9080 - accuracy: 0.3957 - val_loss: 1.7154 - val_accuracy: 0.6826\n",
      "Epoch 2/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 1.6139 - accuracy: 0.6886 - val_loss: 1.2976 - val_accuracy: 0.7801\n",
      "Epoch 3/50\n",
      "192/192 [==============================] - 0s 3ms/step - loss: 1.2026 - accuracy: 0.7815 - val_loss: 0.9615 - val_accuracy: 0.8377\n",
      "Epoch 4/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.9047 - accuracy: 0.8327 - val_loss: 0.7524 - val_accuracy: 0.8639\n",
      "Epoch 5/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.7132 - accuracy: 0.8544 - val_loss: 0.6163 - val_accuracy: 0.8789\n",
      "Epoch 6/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.5905 - accuracy: 0.8748 - val_loss: 0.5214 - val_accuracy: 0.8940\n",
      "Epoch 7/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.4997 - accuracy: 0.8932 - val_loss: 0.4515 - val_accuracy: 0.9090\n",
      "Epoch 8/50\n",
      "192/192 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.9043 - val_loss: 0.3982 - val_accuracy: 0.9162\n",
      "Epoch 9/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.3789 - accuracy: 0.9244 - val_loss: 0.3563 - val_accuracy: 0.9260\n",
      "Epoch 10/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.3293 - accuracy: 0.9405 - val_loss: 0.3226 - val_accuracy: 0.9339\n",
      "Epoch 11/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.2930 - accuracy: 0.9428 - val_loss: 0.2949 - val_accuracy: 0.9372\n",
      "Epoch 12/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.2660 - accuracy: 0.9480 - val_loss: 0.2725 - val_accuracy: 0.9365\n",
      "Epoch 13/50\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.2370 - accuracy: 0.9554 - val_loss: 0.2534 - val_accuracy: 0.9398\n",
      "Epoch 14/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.2166 - accuracy: 0.9611 - val_loss: 0.2377 - val_accuracy: 0.9411\n",
      "Epoch 15/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9665 - val_loss: 0.2240 - val_accuracy: 0.9411\n",
      "Epoch 16/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1817 - accuracy: 0.9678 - val_loss: 0.2130 - val_accuracy: 0.9431\n",
      "Epoch 17/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9683 - val_loss: 0.2033 - val_accuracy: 0.9444\n",
      "Epoch 18/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1505 - accuracy: 0.9748 - val_loss: 0.1949 - val_accuracy: 0.9431\n",
      "Epoch 19/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1391 - accuracy: 0.9764 - val_loss: 0.1880 - val_accuracy: 0.9463\n",
      "Epoch 20/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1284 - accuracy: 0.9794 - val_loss: 0.1821 - val_accuracy: 0.9450\n",
      "Epoch 21/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1176 - accuracy: 0.9802 - val_loss: 0.1767 - val_accuracy: 0.9463\n",
      "Epoch 22/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1093 - accuracy: 0.9827 - val_loss: 0.1720 - val_accuracy: 0.9457\n",
      "Epoch 23/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.1039 - accuracy: 0.9804 - val_loss: 0.1680 - val_accuracy: 0.9444\n",
      "Epoch 24/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9845 - val_loss: 0.1642 - val_accuracy: 0.9457\n",
      "Epoch 25/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0912 - accuracy: 0.9850 - val_loss: 0.1610 - val_accuracy: 0.9457\n",
      "Epoch 26/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0845 - accuracy: 0.9854 - val_loss: 0.1584 - val_accuracy: 0.9444\n",
      "Epoch 27/50\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.0805 - accuracy: 0.9873 - val_loss: 0.1564 - val_accuracy: 0.9450\n",
      "Epoch 28/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0749 - accuracy: 0.9886 - val_loss: 0.1540 - val_accuracy: 0.9450\n",
      "Epoch 29/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9885 - val_loss: 0.1523 - val_accuracy: 0.9463\n",
      "Epoch 30/50\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.0657 - accuracy: 0.9919 - val_loss: 0.1511 - val_accuracy: 0.9444\n",
      "Epoch 31/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9919 - val_loss: 0.1498 - val_accuracy: 0.9457\n",
      "Epoch 32/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0566 - accuracy: 0.9942 - val_loss: 0.1485 - val_accuracy: 0.9463\n",
      "Epoch 33/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0535 - accuracy: 0.9916 - val_loss: 0.1475 - val_accuracy: 0.9450\n",
      "Epoch 34/50\n",
      "192/192 [==============================] - 1s 4ms/step - loss: 0.0539 - accuracy: 0.9904 - val_loss: 0.1460 - val_accuracy: 0.9463\n",
      "Epoch 35/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0498 - accuracy: 0.9939 - val_loss: 0.1452 - val_accuracy: 0.9463\n",
      "Epoch 36/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0457 - accuracy: 0.9956 - val_loss: 0.1454 - val_accuracy: 0.9457\n",
      "Epoch 37/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9964 - val_loss: 0.1452 - val_accuracy: 0.9457\n",
      "Epoch 38/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0386 - accuracy: 0.9961 - val_loss: 0.1450 - val_accuracy: 0.9457\n",
      "Epoch 39/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0376 - accuracy: 0.9968 - val_loss: 0.1455 - val_accuracy: 0.9450\n",
      "Epoch 40/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0385 - accuracy: 0.9942 - val_loss: 0.1460 - val_accuracy: 0.9450\n",
      "Epoch 41/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 0.9954 - val_loss: 0.1447 - val_accuracy: 0.9457\n",
      "Epoch 42/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0339 - accuracy: 0.9949 - val_loss: 0.1451 - val_accuracy: 0.9450\n",
      "Epoch 43/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0319 - accuracy: 0.9971 - val_loss: 0.1451 - val_accuracy: 0.9457\n",
      "Epoch 44/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9958 - val_loss: 0.1455 - val_accuracy: 0.9457\n",
      "Epoch 45/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9951 - val_loss: 0.1471 - val_accuracy: 0.9463\n",
      "Epoch 46/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0261 - accuracy: 0.9969 - val_loss: 0.1470 - val_accuracy: 0.9470\n",
      "Epoch 47/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0238 - accuracy: 0.9970 - val_loss: 0.1487 - val_accuracy: 0.9457\n",
      "Epoch 48/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0246 - accuracy: 0.9960 - val_loss: 0.1497 - val_accuracy: 0.9437\n",
      "Epoch 49/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0227 - accuracy: 0.9963 - val_loss: 0.1505 - val_accuracy: 0.9437\n",
      "Epoch 50/50\n",
      "192/192 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.1508 - val_accuracy: 0.9450\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "# trener modellen på datasettet, 50 ganger\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32/32 [==============================] - 2s 45ms/step - loss: 0.1877 - accuracy: 0.9320\n",
      "Loss:  0.18773253262043\n",
      "Accuracy:  0.9319999814033508\n"
     ]
    }
   ],
   "source": [
    "# tester hvor bra modellen er med test datasettet\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eksporterer den ferdigtrente modellen så vi kan sette inn våre egne input\n",
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'preprocess_bert_input' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6e004f503c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"> \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# deler inputtet inn i orddeler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_bert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# kjører modellen på inputtet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_bert_input' is not defined"
     ]
    }
   ],
   "source": [
    "# nå kan vi teste egne input!\n",
    "inp = input(\"> \")\n",
    "# deler inputtet inn i orddeler\n",
    "inp = preprocess_bert_input(inp)\n",
    "# kjører modellen på inputtet\n",
    "result = export_model.predict([inp])[0]\n",
    "\n",
    "# printer resultatet\n",
    "best_index = 0\n",
    "best_score = 0\n",
    "for i in range(len(result)):\n",
    "    print(f\"{raw_train_ds.class_names[i]}: {result[i]}\")\n",
    "    if result[i] > best_score:\n",
    "        best_score = result[i]\n",
    "        best_index = i\n",
    "print(\"Result: \" + raw_train_ds.class_names[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.save('sprakmodell.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'raw_train_ds' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7cccb4af956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_train_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_train_ds' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}