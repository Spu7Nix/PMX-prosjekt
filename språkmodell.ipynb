{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cbf51a2a14c586d5291331e7296f10f4c57288ef364409bd6d5208dc30c7c035"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteker\n",
    "\n",
    "\"\"\"\n",
    "bruker tensorflow nightly:\n",
    "\n",
    "pip3 install -q tensorflow_text_nightly --user\n",
    "pip3 install -q tf-nightly --user\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 7649 files belonging to 7 classes.\n",
      "Using 6120 files for training.\n",
      "Found 7649 files belonging to 7 classes.\n",
      "Using 1529 files for validation.\n",
      "Found 1595 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# importerer og blander dataen\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# splitter treningsdataen i to, en til trening og en til validering av treningen\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"./data/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/test',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skaffer vokabularet vi har laget\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "bert_vocab_args = dict(\n",
    "    # maksimum størrelse for vokabularet\n",
    "    vocab_size = 8000 * 7,\n",
    "    # Reserverte orddeler som må være med\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # flere argumenter\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "# lager en \"tokenizer\", som deler tekst opp i orddeler\n",
    "tokenizer = text.BertTokenizer('vocab.txt', **bert_tokenizer_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjoner som blir brukt til å voktorisere teksten\n",
    "# altså gjøre dem om til tall i stedet for bokstaver\n",
    "# hvert tall er IDen til en orddel fra vokabularet vårt\n",
    "\n",
    "# alle vektorene må være like lange, så vi legger til\n",
    "# \"Padding\" på slutten av vektoren hvis den ikke er lang nok\n",
    "\n",
    "# IDen til padding\n",
    "PAD_ID = 0\n",
    "# Maksimum lengde for vektoren\n",
    "# hvis vektoren er mindre, blir det lagt til padding\n",
    "max_seq_len = 20\n",
    "\n",
    "# denne funskjonen er her for å passe formatet vi skal bruke den i senere\n",
    "def vectorize(text, label):\n",
    "  # text er tekst inputtet, og label er hvilket språk det er på\n",
    "  ids, mask, type_ids = preprocess_bert_input(text)\n",
    "  return (ids, mask, type_ids), label\n",
    "\n",
    "def preprocess_bert_input(text):\n",
    "  # finner IDene til alle orddelene i inputtet\n",
    "  ids = tokenize_text(text, max_seq_len)\n",
    "  # lager en mask, som i dette tilfettet representerer lengden på vektoren vår\n",
    "  mask = tf.cast(ids > 0, tf.int64)\n",
    "  mask = tf.reshape(mask, [-1, max_seq_len])\n",
    "  # lager den ferdige vektoren\n",
    "  # først fyller lager vi en vektor med\n",
    "  # den riktige lengden (shape) fyllt med nuller\n",
    "  zeros_dims = tf.stack(tf.shape(mask))\n",
    "  type_ids = tf.fill(zeros_dims, PAD_ID)\n",
    "  # så setter vi inn de faktiske orddelenes IDer\n",
    "  type_ids = tf.cast(type_ids, tf.int64)\n",
    "\n",
    "  return (ids, mask, type_ids)\n",
    "\n",
    "def tokenize_text(text, seq_len):\n",
    "  # bruker \"tokenizeren\" vi lagde tidligere til å generere tokens som passer teksten\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  # tilpasser outputtet\n",
    "  tokens = tokens.merge_dims(1, 2)[:, :seq_len]\n",
    "\n",
    "  # klipper vekk slutten hvis den er lenger enn maksimum lengde\n",
    "  tokens = tokens[:, :seq_len]\n",
    "  # legger til padding hvis den er kortere enn maksimum lengde\n",
    "  tokens = tokens.to_tensor(default_value=PAD_ID)\n",
    "  pad = seq_len - tf.shape(tokens)[1]\n",
    "  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n",
    "  return tf.reshape(tokens, [-1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vektoriserer hvert datasett, med funksjonen vi lagde med det spesielle formatet\n",
    "train_ds = raw_train_ds.map(vectorize)\n",
    "val_ds = raw_val_ds.map(vectorize)\n",
    "test_ds = raw_test_ds.map(vectorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tekst:  tf.Tensor(b'turkiet aer en viktig regional makt i denna del av eurasien med kulturellt och ekonomiskt inflytande i omraadet mellan adriatiska havet i vaester och kina i oester ryssland i norr och mellanoestern i soeder och har kommit att faa en oekande strategisk betydelse18 procent av turkiets befolkning beraeknas vara kurder men siffran har aeven uppskattats vara runt 20 procent', shape=(), dtype=string)\n\nSpråk:  sv\n\nOrddeler:  turkiet a ##er en viktig regional makt i denna del av eu ##ras ##ien med kultur ##ellt och e ##konomi\n"
     ]
    }
   ],
   "source": [
    "# printer en bit av dataen for å se at det fungerer\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Tekst: \", first_review)\n",
    "print(\"\\nSpråk: \", raw_train_ds.class_names[first_label])\n",
    "vocab = [w.strip() for w in open(\"vocab.txt\", encoding=\"utf-8\").readlines()]\n",
    "print(\"\\nOrddeler: \", \n",
    "    \" \".join(vocab[id] for id in vectorize(first_review, first_label)[0][0][0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimaliserer dataen (er ærlig talt ikke sikker på hva dette gjør men alle andre prosjekter gjør det)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 32)            320000    \n_________________________________________________________________\ndropout (Dropout)            (None, 20, 32)            0         \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 32)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense (Dense)                (None, 7)                 231       \n=================================================================\nTotal params: 320,231\nTrainable params: 320,231\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# designer modellen som skal bli lært\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  # omformer inputtet til tall som er \"bedre å lære med\"\n",
    "  # i stedet for et heltall som ID for hver orddel, blir det til en liste\n",
    "  # med nuller for hvert språk det ikke er og en 1 for det språket som er riktig\n",
    "  # dette gjør at modellen ikke tror at orddeler som har tall nærme hverandre likner hverandre\n",
    "  layers.Embedding(10000, 32, input_length=max_seq_len),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # passer på at dataen forstsatt har lik lengde\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # output (en for hvert språk, som det er 7 av)\n",
    "  layers.Dense(7)])\n",
    "\n",
    "# printer ut modellen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompilerer modellen\n",
    "# optimisereren er en funksjon som proøver å forbedre modellen\n",
    "# i hver iterasjon av læringen\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(\n",
    "    # loss funksjonen beregner hvor langt unna det riktige svaret modellen er\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=optimizer, metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0539 - accuracy: 0.9931 - val_loss: 0.1674 - val_accuracy: 0.9372\n",
      "Epoch 2/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0514 - accuracy: 0.9925 - val_loss: 0.1676 - val_accuracy: 0.9379\n",
      "Epoch 3/30\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0459 - accuracy: 0.9941 - val_loss: 0.1664 - val_accuracy: 0.9366\n",
      "Epoch 4/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0420 - accuracy: 0.9954 - val_loss: 0.1675 - val_accuracy: 0.9359\n",
      "Epoch 5/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0401 - accuracy: 0.9956 - val_loss: 0.1675 - val_accuracy: 0.9353\n",
      "Epoch 6/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0350 - accuracy: 0.9972 - val_loss: 0.1686 - val_accuracy: 0.9353\n",
      "Epoch 7/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0346 - accuracy: 0.9966 - val_loss: 0.1684 - val_accuracy: 0.9353\n",
      "Epoch 8/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0315 - accuracy: 0.9967 - val_loss: 0.1694 - val_accuracy: 0.9353\n",
      "Epoch 9/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0301 - accuracy: 0.9966 - val_loss: 0.1695 - val_accuracy: 0.9353\n",
      "Epoch 10/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0266 - accuracy: 0.9979 - val_loss: 0.1712 - val_accuracy: 0.9346\n",
      "Epoch 11/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0258 - accuracy: 0.9974 - val_loss: 0.1722 - val_accuracy: 0.9346\n",
      "Epoch 12/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0235 - accuracy: 0.9984 - val_loss: 0.1729 - val_accuracy: 0.9346\n",
      "Epoch 13/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0219 - accuracy: 0.9985 - val_loss: 0.1756 - val_accuracy: 0.9346\n",
      "Epoch 14/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.1784 - val_accuracy: 0.9346\n",
      "Epoch 15/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0186 - accuracy: 0.9982 - val_loss: 0.1801 - val_accuracy: 0.9339\n",
      "Epoch 16/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0183 - accuracy: 0.9979 - val_loss: 0.1824 - val_accuracy: 0.9339\n",
      "Epoch 17/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0162 - accuracy: 0.9980 - val_loss: 0.1830 - val_accuracy: 0.9353\n",
      "Epoch 18/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0162 - accuracy: 0.9984 - val_loss: 0.1847 - val_accuracy: 0.9339\n",
      "Epoch 19/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 0.9984 - val_loss: 0.1864 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0131 - accuracy: 0.9987 - val_loss: 0.1882 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0127 - accuracy: 0.9990 - val_loss: 0.1903 - val_accuracy: 0.9339\n",
      "Epoch 22/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.1937 - val_accuracy: 0.9353\n",
      "Epoch 23/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0114 - accuracy: 0.9989 - val_loss: 0.1948 - val_accuracy: 0.9339\n",
      "Epoch 24/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0099 - accuracy: 0.9993 - val_loss: 0.1967 - val_accuracy: 0.9326\n",
      "Epoch 25/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0102 - accuracy: 0.9989 - val_loss: 0.2002 - val_accuracy: 0.9339\n",
      "Epoch 26/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0093 - accuracy: 0.9989 - val_loss: 0.2027 - val_accuracy: 0.9326\n",
      "Epoch 27/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 0.9992 - val_loss: 0.2056 - val_accuracy: 0.9326\n",
      "Epoch 28/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0082 - accuracy: 0.9993 - val_loss: 0.2086 - val_accuracy: 0.9326\n",
      "Epoch 29/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0080 - accuracy: 0.9989 - val_loss: 0.2112 - val_accuracy: 0.9326\n",
      "Epoch 30/30\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 0.2133 - val_accuracy: 0.9307\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "# trener modellen på datasettet, 50 ganger\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50/50 [==============================] - 3s 45ms/step - loss: 0.2323 - accuracy: 0.9279\n",
      "Loss:  0.23226961493492126\n",
      "Accuracy:  0.9278996586799622\n"
     ]
    }
   ],
   "source": [
    "# tester hvor bra modellen er med test datasettet\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eksporterer den ferdigtrente modellen så vi kan sette inn våre egne input\n",
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "da: 0.867804765701294\nen: 0.06320694088935852\nes: 0.353193461894989\nja: 0.1558380424976349\nnb: 0.9753385782241821\nnn: 0.08760064840316772\nsv: 0.07165762782096863\nResult: nb\n"
     ]
    }
   ],
   "source": [
    "# nå kan vi teste egne input!\n",
    "import unidecode\n",
    "inp = input(\"> \")\n",
    "\n",
    "# fjern linjeskift\n",
    "inp = inp.replace('\\n', ' ').lower()\n",
    "# erstatt æøå osv\n",
    "inp = inp.replace('æ', 'ae').replace(\n",
    "    'å', 'aa').replace('ø', 'oe').replace('ö', 'oe').replace('ä', 'ae').replace('ä', 'ae')\n",
    "inp = unidecode.unidecode(inp)\n",
    "# fjern noen tall wikipedia setter inn for kilder eller noe\n",
    "inp = re.sub(r\"\\[[0-9]*\\]\", \"\", inp)\n",
    "# fjern tegnsetting\n",
    "inp = inp.translate(str.maketrans('', '', string.punctuation))\n",
    "# deler inputtet inn i orddeler\n",
    "tok = preprocess_bert_input(inp)\n",
    "# kjører modellen på inputtet\n",
    "result = export_model.predict([tok])[0]\n",
    "\n",
    "# printer resultatet\n",
    "best_index = 0\n",
    "best_score = 0\n",
    "for i in range(len(result)):\n",
    "    print(f\"{raw_train_ds.class_names[i]}: {result[i]}\")\n",
    "    if result[i] > best_score:\n",
    "        best_score = result[i]\n",
    "        best_index = i\n",
    "print(\"Result: \" + raw_train_ds.class_names[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['da', 'en', 'es', 'ja', 'nb', 'nn', 'sv']\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.save(\"sprakmodell.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}