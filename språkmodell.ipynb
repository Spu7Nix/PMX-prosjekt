{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cbf51a2a14c586d5291331e7296f10f4c57288ef364409bd6d5208dc30c7c035"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteker\n",
    "\n",
    "\"\"\"\n",
    "bruker tensorflow nightly:\n",
    "\n",
    "pip3 install -q tensorflow_text_nightly --user\n",
    "pip3 install -q tf-nightly --user\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as text\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 7641 files belonging to 7 classes.\n",
      "Using 6113 files for training.\n",
      "Found 7641 files belonging to 7 classes.\n",
      "Using 1528 files for validation.\n",
      "Found 1000 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# importerer og blander dataen\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# splitter treningsdataen i to, en til trening og en til validering av treningen\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"./data/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    './data/test',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skaffer vokabularet vi har laget\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "bert_vocab_args = dict(\n",
    "    # maksimum størrelse for vokabularet\n",
    "    vocab_size = 8000 * 7,\n",
    "    # Reserverte orddeler som må være med\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # flere argumenter\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "# lager en \"tokenizer\", som deler tekst opp i orddeler\n",
    "tokenizer = text.BertTokenizer('vocab.txt', **bert_tokenizer_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjoner som blir brukt til å voktorisere teksten\n",
    "# altså gjøre dem om til tall i stedet for bokstaver\n",
    "# hvert tall er IDen til en orddel fra vokabularet vårt\n",
    "\n",
    "# alle vektorene må være like lange, så vi legger til\n",
    "# \"Padding\" på slutten av vektoren hvis den ikke er lang nok\n",
    "\n",
    "# IDen til padding\n",
    "PAD_ID = 0\n",
    "# Maksimum lengde for vektoren\n",
    "# hvis vektoren er mindre, blir det lagt til padding\n",
    "max_seq_len = 20\n",
    "\n",
    "# denne funskjonen er her for å passe formatet vi skal bruke den i senere\n",
    "def vectorize(text, label):\n",
    "  # text er tekst inputtet, og label er hvilket språk det er på\n",
    "  ids, mask, type_ids = preprocess_bert_input(text)\n",
    "  return (ids, mask, type_ids), label\n",
    "\n",
    "def preprocess_bert_input(text):\n",
    "  # finner IDene til alle orddelene i inputtet\n",
    "  ids = tokenize_text(text, max_seq_len)\n",
    "  # lager en mask, som i dette tilfettet representerer lengden på vektoren vår\n",
    "  mask = tf.cast(ids > 0, tf.int64)\n",
    "  mask = tf.reshape(mask, [-1, max_seq_len])\n",
    "  # lager den ferdige vektoren\n",
    "  # først fyller lager vi en vektor med\n",
    "  # den riktige lengden (shape) fyllt med nuller\n",
    "  zeros_dims = tf.stack(tf.shape(mask))\n",
    "  type_ids = tf.fill(zeros_dims, PAD_ID)\n",
    "  # så setter vi inn de faktiske orddelenes IDer\n",
    "  type_ids = tf.cast(type_ids, tf.int64)\n",
    "\n",
    "  return (ids, mask, type_ids)\n",
    "\n",
    "def tokenize_text(text, seq_len):\n",
    "  # bruker \"tokenizeren\" vi lagde tidligere til å generere tokens som passer teksten\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  # tilpasser outputtet\n",
    "  tokens = tokens.merge_dims(1, 2)[:, :seq_len]\n",
    "\n",
    "  # klipper vekk slutten hvis den er lenger enn maksimum lengde\n",
    "  tokens = tokens[:, :seq_len]\n",
    "  # legger til padding hvis den er kortere enn maksimum lengde\n",
    "  tokens = tokens.to_tensor(default_value=PAD_ID)\n",
    "  pad = seq_len - tf.shape(tokens)[1]\n",
    "  tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n",
    "  return tf.reshape(tokens, [-1, seq_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vektoriserer hvert datasett, med funksjonen vi lagde med det spesielle formatet\n",
    "train_ds = raw_train_ds.map(vectorize)\n",
    "val_ds = raw_val_ds.map(vectorize)\n",
    "test_ds = raw_test_ds.map(vectorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tekst:  tf.Tensor(b'den koloniale bandet blei i r\\xc3\\xb8ynda brote i 1808 d\\xc3\\xa5 hovudstaden i det portugisiske koloniriket blei overf\\xc3\\xb8rt fr\\xc3\\xa5 lisboa til rio de janeiro etter napoleon sin invasjon av portugal', shape=(), dtype=string)\n\nSpråk:  nn\n\nOrddeler:  den k ##ol ##oni ##ale band ##et blei i r ##øy ##nd ##a b ##ro ##te i 18 ##0 ##8\n"
     ]
    }
   ],
   "source": [
    "# printer en bit av dataen for å se at det fungerer\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Tekst: \", first_review)\n",
    "print(\"\\nSpråk: \", raw_train_ds.class_names[first_label])\n",
    "vocab = [w.strip() for w in open(\"vocab.txt\", encoding=\"utf-8\").readlines()]\n",
    "print(\"\\nOrddeler: \", \n",
    "    \" \".join(vocab[id] for id in vectorize(first_review, first_label)[0][0][0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimaliserer dataen (er ærlig talt ikke sikker på hva dette gjør men alle andre prosjekter gjør det)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 20, 32)            320000    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 20, 32)            0         \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 32)                0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 7)                 231       \n=================================================================\nTotal params: 320,231\nTrainable params: 320,231\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# designer modellen som skal bli lært\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  # omformer inputtet til tall som er \"bedre å lære med\"\n",
    "  # i stedet for et heltall som ID for hver orddel, blir det til en liste\n",
    "  # med nuller for hvert språk det ikke er og en 1 for det språket som er riktig\n",
    "  # dette gjør at modellen ikke tror at orddeler som har tall nærme hverandre likner hverandre\n",
    "  layers.Embedding(10000, 32, input_length=max_seq_len),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # passer på at dataen forstsatt har lik lengde\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  # del av netverket som blir trent\n",
    "  layers.Dropout(0.2),\n",
    "  # output (en for hvert språk, som det er 7 av)\n",
    "  layers.Dense(7)])\n",
    "\n",
    "# printer ut modellen\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kompilerer modellen\n",
    "# optimisereren er en funksjon som proøver å forbedre modellen\n",
    "# i hver iterasjon av læringen\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(\n",
    "    # loss funksjonen beregner hvor langt unna det riktige svaret modellen er\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "    optimizer=optimizer, metrics = [\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "192/192 [==============================] - 14s 70ms/step - loss: 1.8833 - accuracy: 0.4553 - val_loss: 1.5649 - val_accuracy: 0.7454\n",
      "Epoch 2/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 1.4172 - accuracy: 0.7633 - val_loss: 1.0215 - val_accuracy: 0.8370\n",
      "Epoch 3/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.9257 - accuracy: 0.8369 - val_loss: 0.7179 - val_accuracy: 0.8639\n",
      "Epoch 4/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.6644 - accuracy: 0.8637 - val_loss: 0.5519 - val_accuracy: 0.8861\n",
      "Epoch 5/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.5099 - accuracy: 0.8958 - val_loss: 0.4475 - val_accuracy: 0.9103\n",
      "Epoch 6/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.4090 - accuracy: 0.9242 - val_loss: 0.3774 - val_accuracy: 0.9254\n",
      "Epoch 7/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.3317 - accuracy: 0.9419 - val_loss: 0.3270 - val_accuracy: 0.9306\n",
      "Epoch 8/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.2827 - accuracy: 0.9539 - val_loss: 0.2889 - val_accuracy: 0.9352\n",
      "Epoch 9/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.2427 - accuracy: 0.9552 - val_loss: 0.2600 - val_accuracy: 0.9372\n",
      "Epoch 10/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.2097 - accuracy: 0.9649 - val_loss: 0.2369 - val_accuracy: 0.9444\n",
      "Epoch 11/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.1830 - accuracy: 0.9675 - val_loss: 0.2192 - val_accuracy: 0.9444\n",
      "Epoch 12/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.1617 - accuracy: 0.9717 - val_loss: 0.2050 - val_accuracy: 0.9437\n",
      "Epoch 13/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.1417 - accuracy: 0.9777 - val_loss: 0.1937 - val_accuracy: 0.9450\n",
      "Epoch 14/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.1252 - accuracy: 0.9834 - val_loss: 0.1844 - val_accuracy: 0.9463\n",
      "Epoch 15/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.1096 - accuracy: 0.9847 - val_loss: 0.1768 - val_accuracy: 0.9490\n",
      "Epoch 16/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0980 - accuracy: 0.9845 - val_loss: 0.1709 - val_accuracy: 0.9483\n",
      "Epoch 17/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0897 - accuracy: 0.9853 - val_loss: 0.1659 - val_accuracy: 0.9476\n",
      "Epoch 18/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0825 - accuracy: 0.9871 - val_loss: 0.1615 - val_accuracy: 0.9483\n",
      "Epoch 19/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0779 - accuracy: 0.9891 - val_loss: 0.1584 - val_accuracy: 0.9463\n",
      "Epoch 20/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0669 - accuracy: 0.9927 - val_loss: 0.1556 - val_accuracy: 0.9470\n",
      "Epoch 21/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0620 - accuracy: 0.9896 - val_loss: 0.1535 - val_accuracy: 0.9457\n",
      "Epoch 22/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0566 - accuracy: 0.9930 - val_loss: 0.1516 - val_accuracy: 0.9470\n",
      "Epoch 23/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0525 - accuracy: 0.9948 - val_loss: 0.1495 - val_accuracy: 0.9476\n",
      "Epoch 24/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0475 - accuracy: 0.9960 - val_loss: 0.1489 - val_accuracy: 0.9463\n",
      "Epoch 25/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0439 - accuracy: 0.9967 - val_loss: 0.1486 - val_accuracy: 0.9470\n",
      "Epoch 26/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0401 - accuracy: 0.9964 - val_loss: 0.1477 - val_accuracy: 0.9470\n",
      "Epoch 27/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0378 - accuracy: 0.9964 - val_loss: 0.1476 - val_accuracy: 0.9463\n",
      "Epoch 28/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0328 - accuracy: 0.9978 - val_loss: 0.1479 - val_accuracy: 0.9470\n",
      "Epoch 29/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0304 - accuracy: 0.9974 - val_loss: 0.1483 - val_accuracy: 0.9450\n",
      "Epoch 30/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0273 - accuracy: 0.9977 - val_loss: 0.1481 - val_accuracy: 0.9450\n",
      "Epoch 31/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0267 - accuracy: 0.9974 - val_loss: 0.1501 - val_accuracy: 0.9437\n",
      "Epoch 32/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0245 - accuracy: 0.9989 - val_loss: 0.1506 - val_accuracy: 0.9437\n",
      "Epoch 33/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0235 - accuracy: 0.9972 - val_loss: 0.1502 - val_accuracy: 0.9463\n",
      "Epoch 34/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0209 - accuracy: 0.9982 - val_loss: 0.1516 - val_accuracy: 0.9476\n",
      "Epoch 35/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0190 - accuracy: 0.9982 - val_loss: 0.1535 - val_accuracy: 0.9450\n",
      "Epoch 36/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0180 - accuracy: 0.9991 - val_loss: 0.1552 - val_accuracy: 0.9450\n",
      "Epoch 37/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0177 - accuracy: 0.9979 - val_loss: 0.1556 - val_accuracy: 0.9450\n",
      "Epoch 38/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0154 - accuracy: 0.9987 - val_loss: 0.1568 - val_accuracy: 0.9457\n",
      "Epoch 39/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0140 - accuracy: 0.9984 - val_loss: 0.1577 - val_accuracy: 0.9457\n",
      "Epoch 40/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0134 - accuracy: 0.9984 - val_loss: 0.1584 - val_accuracy: 0.9463\n",
      "Epoch 41/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0119 - accuracy: 0.9990 - val_loss: 0.1608 - val_accuracy: 0.9457\n",
      "Epoch 42/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0113 - accuracy: 0.9994 - val_loss: 0.1638 - val_accuracy: 0.9450\n",
      "Epoch 43/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0105 - accuracy: 0.9991 - val_loss: 0.1650 - val_accuracy: 0.9450\n",
      "Epoch 44/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0094 - accuracy: 0.9991 - val_loss: 0.1664 - val_accuracy: 0.9444\n",
      "Epoch 45/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0098 - accuracy: 0.9987 - val_loss: 0.1680 - val_accuracy: 0.9444\n",
      "Epoch 46/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.1689 - val_accuracy: 0.9437\n",
      "Epoch 47/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0090 - accuracy: 0.9989 - val_loss: 0.1704 - val_accuracy: 0.9450\n",
      "Epoch 48/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0071 - accuracy: 0.9997 - val_loss: 0.1715 - val_accuracy: 0.9450\n",
      "Epoch 49/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.1740 - val_accuracy: 0.9470\n",
      "Epoch 50/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.1768 - val_accuracy: 0.9476\n",
      "Epoch 51/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0070 - accuracy: 0.9991 - val_loss: 0.1778 - val_accuracy: 0.9444\n",
      "Epoch 52/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9995 - val_loss: 0.1797 - val_accuracy: 0.9444\n",
      "Epoch 53/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.1806 - val_accuracy: 0.9457\n",
      "Epoch 54/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0058 - accuracy: 0.9993 - val_loss: 0.1836 - val_accuracy: 0.9470\n",
      "Epoch 55/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 0.1855 - val_accuracy: 0.9463\n",
      "Epoch 56/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.1870 - val_accuracy: 0.9470\n",
      "Epoch 57/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.1888 - val_accuracy: 0.9463\n",
      "Epoch 58/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1923 - val_accuracy: 0.9457\n",
      "Epoch 59/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.1949 - val_accuracy: 0.9444\n",
      "Epoch 60/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.1972 - val_accuracy: 0.9450\n",
      "Epoch 61/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.1973 - val_accuracy: 0.9457\n",
      "Epoch 62/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.1995 - val_accuracy: 0.9470\n",
      "Epoch 63/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.2021 - val_accuracy: 0.9470\n",
      "Epoch 64/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.2044 - val_accuracy: 0.9444\n",
      "Epoch 65/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.2050 - val_accuracy: 0.9470\n",
      "Epoch 66/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.2070 - val_accuracy: 0.9470\n",
      "Epoch 67/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.2078 - val_accuracy: 0.9476\n",
      "Epoch 68/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.2078 - val_accuracy: 0.9457\n",
      "Epoch 69/100\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.2123 - val_accuracy: 0.9437\n",
      "Epoch 70/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.2125 - val_accuracy: 0.9450\n",
      "Epoch 71/100\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.2161 - val_accuracy: 0.9450\n",
      "Epoch 72/100\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.2165 - val_accuracy: 0.9463\n",
      "Epoch 73/100\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.2187 - val_accuracy: 0.9437\n",
      "Epoch 74/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2218 - val_accuracy: 0.9424\n",
      "Epoch 75/100\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.2235 - val_accuracy: 0.9424\n",
      "Epoch 76/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2242 - val_accuracy: 0.9437\n",
      "Epoch 77/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9989 - val_loss: 0.2255 - val_accuracy: 0.9431\n",
      "Epoch 78/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2276 - val_accuracy: 0.9437\n",
      "Epoch 79/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.2316 - val_accuracy: 0.9424\n",
      "Epoch 80/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.2318 - val_accuracy: 0.9444\n",
      "Epoch 81/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.2306 - val_accuracy: 0.9437\n",
      "Epoch 82/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.2335 - val_accuracy: 0.9437\n",
      "Epoch 83/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9989 - val_loss: 0.2360 - val_accuracy: 0.9450\n",
      "Epoch 84/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.2358 - val_accuracy: 0.9437\n",
      "Epoch 85/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.2391 - val_accuracy: 0.9437\n",
      "Epoch 86/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9989 - val_loss: 0.2382 - val_accuracy: 0.9431\n",
      "Epoch 87/100\n",
      "192/192 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2394 - val_accuracy: 0.9431\n",
      "Epoch 88/100\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.2416 - val_accuracy: 0.9424\n",
      "Epoch 89/100\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.2429 - val_accuracy: 0.9424\n",
      "Epoch 90/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.2446 - val_accuracy: 0.9437\n",
      "Epoch 91/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.2490 - val_accuracy: 0.9424\n",
      "Epoch 92/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.2492 - val_accuracy: 0.9437\n",
      "Epoch 93/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.2504 - val_accuracy: 0.9424\n",
      "Epoch 94/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.2528 - val_accuracy: 0.9424\n",
      "Epoch 95/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2549 - val_accuracy: 0.9457\n",
      "Epoch 96/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.2581 - val_accuracy: 0.9431\n",
      "Epoch 97/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.2583 - val_accuracy: 0.9437\n",
      "Epoch 98/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 0.2600 - val_accuracy: 0.9437\n",
      "Epoch 99/100\n",
      "192/192 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2618 - val_accuracy: 0.9444\n",
      "Epoch 100/100\n",
      "192/192 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.2605 - val_accuracy: 0.9418\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "# trener modellen på datasettet, 50 ganger\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32/32 [==============================] - 2s 44ms/step - loss: 0.3281 - accuracy: 0.9290\n",
      "Loss:  0.3280984163284302\n",
      "Accuracy:  0.9290000200271606\n"
     ]
    }
   ],
   "source": [
    "# tester hvor bra modellen er med test datasettet\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eksporterer den ferdigtrente modellen så vi kan sette inn våre egne input\n",
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "da: 0.7251596450805664\nen: 0.4100765287876129\nes: 0.04229751229286194\nja: 0.7346693277359009\nnb: 0.7064176201820374\nnn: 0.2817928194999695\nsv: 0.21558687090873718\nResult: ja\n"
     ]
    }
   ],
   "source": [
    "# nå kan vi teste egne input!\n",
    "\n",
    "inp = input(\"> \")\n",
    "# deler inputtet inn i orddeler\n",
    "inp = preprocess_bert_input(inp)\n",
    "# kjører modellen på inputtet\n",
    "result = export_model.predict([inp])[0]\n",
    "\n",
    "# printer resultatet\n",
    "best_index = 0\n",
    "best_score = 0\n",
    "for i in range(len(result)):\n",
    "    print(f\"{raw_train_ds.class_names[i]}: {result[i]}\")\n",
    "    if result[i] > best_score:\n",
    "        best_score = result[i]\n",
    "        best_index = i\n",
    "print(\"Result: \" + raw_train_ds.class_names[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['da', 'en', 'es', 'ja', 'nb', 'nn', 'sv']\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.save(\"sprakmodell.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}